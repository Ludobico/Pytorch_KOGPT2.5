{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 허브에는 20652개의 데이터셋이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "all_datasets = list_datasets()\n",
    "print('현재 허브에는 {0}개의 데이터셋이 있습니다.'.format(len(all_datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (C:/Users/aqs45/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "100%|██████████| 3/3 [00:00<00:00, 750.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotion = load_dataset('emotion') # emotion 데이터셋 로드\n",
    "emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = emotion['train']\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy'], 'label': [0, 0, 3, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy']\n"
     ]
    }
   ],
   "source": [
    "print(train_ds['text'][:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허브에 필요한 데이터셋이 없다면\n",
    "\n",
    "이 책의 대부분 예제는 허깅페이스 허브를 사용해 데이터셋을 다운로드합니다. 하지만 많은 경우, 노트북 컴퓨터나 회사 원격 서버에 저장된 데이터를 사용해 작업합니다. 허깅페이스 데이터셋은 로컬 데이터셋이나 원격 데이터셋에 사용 가능한 로딩 스크립트를 몇 가지 제공합니다.\n",
    "\n",
    "### 포맷에 따른 데이터셋 로딩 방법\n",
    "* CSV : load_dataset('csv', data_files='my_file.csv')\n",
    "* 텍스트 : load_dataset('text', data_files='my_file.txt')\n",
    "* JSON : load_dataset('json', data_files='my_file.jsonl')\n",
    "\n",
    "이렇게 데이터 포맷마다 `load_dataset()` 함수에 연관된 로딩 스크립트를 전달하고 data_files 매개변수에 `파일 경로`나 `URL`을 하나 이상 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotion.set_format(type='pandas')\n",
    "df = emotion['train'][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_int2str(row):\n",
    "    return emotion['train'].features['label'].int2str(row) # int2str() 정수 레이블을 클래스 이름으로 바꿔줌\n",
    "\n",
    "df['label_name'] = df['label'].apply(label_int2str) # apply() 특정 객체의 메소드를 적용\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['label_name'].value_counts(ascending=True).plot.barh()\n",
    "plt.title('Frequency of Classes')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불균형한 데이터는 다음 방법으로 다룹니다.\n",
    "\n",
    "* 소수 클래스를 랜덤하게 오버샘플링\n",
    "* 다수 클래스를 랜덤하게 언더샘플링\n",
    "* 클래스의 대표성이 부족하다면 레이블된 데이터를 더 많이 수집"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트윗 길이 확인\n",
    "\n",
    "트랜스포머 모델은 최대 문맥 크기(maximum context size)라는 최대 입력 시퀀스 길이가 있습니다. DistillBERT를 사용하는 애플리케이션에서 최대 문맥 크기는 512토큰으로 문단 몇 개 정도가 됩니다. 다음 절에서 보겠지만, 토큰은 `텍스트의 기본 단위`입니다. 여기서는 토큰을 단어로 간주하겠습니다. 트윗당 단어 분포를 보면 감정에 따른 트윗 길이가 대략 추정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Words Per Tweet'] = df['text'].str.split().apply(len)\n",
    "df.boxplot('Words Per Tweet', by='label_name', grid=False,\n",
    "           color='black', showfliers=False,)  # showfliers : 아웃라이어는 표시하지 않음\n",
    "plt.suptitle('')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion.reset_format() # dataFrane 포맷이 필요하지 않으니 데이터셋의 출력 포맷을 초기화"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트에서 토큰으로\n",
    "\n",
    "DistillBERT 같은 트랜스포머 모델은 원시 문자열을 입력으로 받지 못합니다. 대신 텍스트가 토큰화되어 수치 벡터로 인코딩됐다고 가정해보죠. 토큰화는 문자열을 모델이 사용하는 기본 단위로 분할하는 단계입니다. 적용할 수 있는 토큰화 전략이 몇 가지 있으며 단어를 부분단위로 나누기 위한 최적 분할은 일반적으로 말뭉치에서 학습됩니다. 문자 토큰화와 단어 토큰화라는 극단적인 두 가지 방식을 살펴본 후 DistillBERT에서 사용하는 토크나이저에 대해 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Tokenizing text is a core task of NLP.'\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Label ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megatron</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Label ID\n",
       "0      Bumblebee         0\n",
       "1  Optimus Prime         1\n",
       "2       Megatron         2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_df = pd.DataFrame(\n",
    "    {'Name' : ['Bumblebee', 'Optimus Prime', 'Megatron'], 'Label ID' : [0,1,2]}\n",
    ")\n",
    "\n",
    "categorical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bumblebee</th>\n",
       "      <th>Megatron</th>\n",
       "      <th>Optimus Prime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bumblebee  Megatron  Optimus Prime\n",
       "0          1         0              0\n",
       "1          0         0              1\n",
       "2          0         1              0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(categorical_df['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5, 14, 12,  8, 13, 11, 19, 11, 13, 10,  0, 17,  8, 18, 17,  0, 11, 16,\n",
      "         0,  6,  0,  7, 14, 15,  8,  0, 17,  6, 16, 12,  0, 14,  9,  0,  3,  2,\n",
      "         4,  1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 20])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.Tensor(input_ids).to(torch.int64)\n",
    "print(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot_encodings.shape # 38개의 입력토큰 각각에 20차원의 원-핫 벡터 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 : T\n",
      "텐서 인덱스: 5\n",
      "원-핫 인코딩: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f'토큰 : {tokenized_text[0]}')\n",
    "print(f'텐서 인덱스: {input_ids[0]}')\n",
    "print(f'원-핫 인코딩: {one_hot_encodings[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화\n",
    "\n",
    "텍스트를 문자가 아니라 단어로 분할하고 각 단어를 정수로 매핑하겠습니다. 처음부터 단어를 사용하면 모델이 문자에서 단어를 학습하는 단계가 생략되어 훈련 과정의 복잡도가 감소합니다.\n",
    "\n",
    "간단한 단어 토크나이저는 공백을 사용해 텍스트를 토큰화하는 것입니다. 원시 텍스트에 파이썬의 `split()` 함수를 적용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoTokenizer\n",
    "\n",
    "AutoTokenizer 클래스는 체크포인트 이름을 사용해 `모델의 설정`, `사전 훈련된 가중치`, `어휘사전`을 자동으로 추출하는 자동 클래스입니다. 이 클래스를 사용하면 모델 간의 빠른 전환이 가능하지만, 특정 클래스를 수동으로 로드할 수도 있습니다. 예를 들어 DistillBERT 토크나이저를 로드하는 방법은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "distillbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)  # convert_ids_to_tokens 메서드를 사용해 다시 토큰으로 변환\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 세 가지 지점을 살펴보겠습니다. 먼저, `[CLS]`와 `[SEP]` 가 시퀀스 처음과 끝에 추가되었습니다. 이런 토큰은 모델마다 다르지만 주요 역할은 `시퀀스의 시작과 끝을 알리는것`입니다.\n",
    "\n",
    "둘째, 토큰이 모두 소문자로 변환됐습니다. 이것이 이 체크포인트의 특징입니다.\n",
    "\n",
    "마지막으로 `tokenizing`과 `NLP` 가 각각 두 개의 토큰으로 나뉘었습니다. 자주 등장하는 단어가 아니기 때문입니다. ##izing과 ##p에 있는 #는 앞의 문자열이 공백이 아님을 뜻합니다. 이런 접두사가 붙은 토큰은 문자열로 다시 바꿀 때 앞의 토큰과 합칩니다. AutoTokenizer 클래스는 이런 작업을 수행하는 `convert_tokens_to_string()` 메서드를 제공합니다. 앞의 토큰에 이 메서드를 적용해보죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size # 어휘사전의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length # 모델의 최대 문맥 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names # 모델의 필드 이름"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 데이터셋 토큰화하기\n",
    "\n",
    "전체 말뭉치를 토큰화하기 위해 DatasetDict 객체의 `map()` 메서드를 사용하겠습니다. 이 책에서는 데이터셋에 있는 각 원소에 어떤 처리 함수를 편리하게 적용하기 위해 map() 메서드를 많이 사용합니다. 곧 보겠지만, map() 메서드는 새 행과 열을 만드는데 사용할 수도 있습니다.\n",
    "\n",
    "먼저 샘플을 토큰화할 처리 함수가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* padding=True 배치에 있는 가장 긴 샘플 크기에 맞춰 샘플을 0으로 패딩\n",
    "\n",
    "* truncation=True 모델의 최대 문맥 크기에 맞춰 샘플을 잘라냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(emotion['train'][:2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패딩 결과를 보겠습니다. input_ids의 첫 번째 원소가 두 번쨰보다 더 짧으므로 길이를 동일하게 맞추기 위해 끝에 0을 추가했습니다. 0은 어휘사전에 있는 [PAD] 토큰에 해당합니다. 특수한 토큰은 앞서 본 [CLS]와 [SEP] 등입니다.\n",
    "\n",
    "|특별한 토큰|[PAD]|[UNK]|[CLS]|[SEP]|[MASK]|\n",
    "|---|---|---|---|---|---|\n",
    "|특별한 토큰 ID|0|100|101|102|103|\n",
    "\n",
    "토크나이저는 인코딩된 트윗을 input_ids로 반환하고, attention_mask 배열 리스트도 반환합니다. 추가된 패딩 토큰 때문에 모델이 혼동하지 않게 하려는 조치입니다.\n",
    "\n",
    "![](./Static/%ED%99%94%EB%A9%B4%20%EC%BA%A1%EC%B2%98%202023-02-07%20143334.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\aqs45\\.cache\\huggingface\\datasets\\emotion\\split\\1.0.0\\cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd\\cache-6316da5de5436960.arrow\n",
      "Loading cached processed dataset at C:\\Users\\aqs45\\.cache\\huggingface\\datasets\\emotion\\split\\1.0.0\\cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd\\cache-2cb11f4abdb48d91.arrow\n",
      "Loading cached processed dataset at C:\\Users\\aqs45\\.cache\\huggingface\\datasets\\emotion\\split\\1.0.0\\cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd\\cache-1983a40220248de7.arrow\n"
     ]
    }
   ],
   "source": [
    "emotions_encoded = emotion.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 `map()` 메서드는 말뭉치에 있는 모든 샘플에 개별적으로 작용하므로, batched=True로 설정하여 트윗을 배치로 인코딩하겠습니다.\n",
    "\n",
    "batch_size=None 으로 설정했기 때문에 전체 데이터셋 하나의 배치로 tokenize() 함수에 적용됩니다. 이렇게 하면 입력 텐서와 어텐션 마스크는 전역적으로 동일한 크기로 생성됩니다. 또 이 연산은 데이터셋에 input_ids와 attention_mask 열을 새로 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(emotions_encoded['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(emotions_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분류 모델 훈련하기\n",
    "\n",
    "DistillBERT 같은 모델은 텍스트 시퀀스에 있는 마스킹된 단어를 예측하도록 사전 훈련합니다. 하지만 이런 언어 모델을 바로 텍스트 분류에 사용하지는 못합니다. 약간의 수정이 필요합니다.\n",
    "\n",
    "![](./Static/143813.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 268M/268M [00:06<00:00, 42.2MB/s] \n",
      "c:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aqs45\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 마지막 은닉 상태 추출하기\n",
    "\n",
    "하나의 문자열에 대한 마지막 은닉 상태를 추출하는 것으로 시작해보죠. 우선 문자열을 인코딩하고 토큰을 파이토치 텐서로 변환합니다. 토크나이저에 return_tensors='pt' 매개변수를 지정해 이 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텐서 크기 : torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "text = 'this is a test'\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "print(f\"입력 텐서 크기 : {inputs['input_ids'].size()}\") # 텐서 결과의 크기는 [batch_size, n_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
      "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
      "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
      "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
      "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
      "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.size() # [batch_size, n_tokens, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "    if k in tokenizer.model_input_names}\n",
    "\n",
    "    # 마지막 은닉 상태 추출\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    \n",
    "    # [CLS] 토큰에 대한 벡터 반환\n",
    "    return {'hidden_state': last_hidden_state[:,:0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?ba/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 3.24 GiB already allocated; 0 bytes free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m emotions_hidden \u001b[39m=\u001b[39m emotions_encoded\u001b[39m.\u001b[39;49mmap(extract_hidden_states, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\datasets\\dataset_dict.py:816\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 816\u001b[0m     {\n\u001b[0;32m    817\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    818\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[0;32m    819\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[0;32m    820\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[0;32m    821\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[0;32m    822\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[0;32m    823\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    824\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    825\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[0;32m    826\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    827\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    828\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    829\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    830\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m    831\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[0;32m    832\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[0;32m    833\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[0;32m    834\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    837\u001b[0m     }\n\u001b[0;32m    838\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\datasets\\dataset_dict.py:817\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    816\u001b[0m     {\n\u001b[1;32m--> 817\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    818\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    819\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    820\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m    821\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    822\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    823\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    824\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m    825\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    826\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    827\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    828\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    829\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    830\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    831\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    832\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    833\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    834\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    837\u001b[0m     }\n\u001b[0;32m    838\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2815\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2812\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   2814\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 2815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   2816\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   2817\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   2818\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   2819\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   2820\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   2821\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2822\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   2823\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   2824\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   2825\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   2826\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   2827\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   2828\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   2829\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   2830\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   2831\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   2832\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   2833\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   2834\u001b[0m     )\n\u001b[0;32m   2835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2837\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:546\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    547\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    548\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    549\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:513\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    507\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    508\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    509\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    510\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    511\u001b[0m }\n\u001b[0;32m    512\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    514\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    515\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\datasets\\fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3236\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   3232\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   3233\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[0;32m   3234\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3236\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   3237\u001b[0m         batch,\n\u001b[0;32m   3238\u001b[0m         indices,\n\u001b[0;32m   3239\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m   3240\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[0;32m   3241\u001b[0m     )\n\u001b[0;32m   3242\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3243\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3244\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3245\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3112\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3110\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   3111\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> 3112\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39madditional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3114\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m   3115\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[0;32m   3116\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[43], line 7\u001b[0m, in \u001b[0;36mextract_hidden_states\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# 마지막 은닉 상태 추출\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m     last_hidden_state \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[0;32m      9\u001b[0m \u001b[39m# [CLS] 토큰에 대한 벡터 반환\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mhidden_state\u001b[39m\u001b[39m'\u001b[39m: last_hidden_state[:,:\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()}\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:579\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    578\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 579\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    580\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    581\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    582\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    583\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    584\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    585\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    586\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:355\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    353\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 355\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    356\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[0;32m    357\u001b[0m )\n\u001b[0;32m    358\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:290\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    291\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    292\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    293\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    294\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    295\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    296\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    297\u001b[0m )\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    299\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Transformers\\venv\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:215\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    212\u001b[0m v \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_lin(value))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    214\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    216\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    217\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(\n\u001b[0;32m    218\u001b[0m     mask, torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mfinfo(scores\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin)\n\u001b[0;32m    219\u001b[0m )  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 3.24 GiB already allocated; 0 bytes free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7409877075998b40c7f75e9adac75e1ccf121f4a98cd2e57d4ea6d51ec765852"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
